{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is initial version for testing multi-class classification. This notebook will be converted to script in future!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six\n",
    "import tensorflow as tf\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import (model_selection, linear_model, multiclass,\n",
    "                     preprocessing)\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads the labels from the node and labels file. logic for multi-label will be added in future!!\n",
    "def load_labels(label_filename, vocab_size):\n",
    "    \"\"\"Load labels file. Supports single or multiple labels\"\"\"\n",
    "    raw_labels = {}\n",
    "    min_labels = np.inf\n",
    "    max_labels = 0\n",
    "    with open(label_filename) as f:\n",
    "        for line in f.readlines():\n",
    "            values = [int(x) for x in line.strip().split()]\n",
    "            raw_labels[values[0]] = values[1:]\n",
    "            min_labels = min(len(values) - 1, min_labels)\n",
    "            max_labels = max(len(values) - 1, max_labels)\n",
    "    print(\"Raw Labels: {}\".format(len(raw_labels)))\n",
    "    if min_labels < 1:\n",
    "        raise RuntimeError(\"Expected 1 or more labels in file {}\"\n",
    "                           .format(label_filename))\n",
    "    # Single label\n",
    "    elif max_labels == 1:\n",
    "        labels = np.full(vocab_size, np.nan, dtype=np.int32)\n",
    "        for (index, label) in six.iteritems(raw_labels):\n",
    "            labels[index] = label[0]\n",
    "        return raw_labels, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function builds the classification model for evaluation of embedding. \n",
    "# Model for multi-label classification will be added in future.\n",
    "def eval_classification(labels, embeddings,seed,train_split):\n",
    "    \n",
    "    classifier = linear_model.LogisticRegression(random_state=seed)\n",
    "    \n",
    "    shuffle = model_selection.StratifiedShuffleSplit(n_splits=10, train_size=train_split, test_size=1.0 - train_split)\n",
    "\n",
    "    scoring = ['accuracy', 'f1_macro', 'f1_micro']\n",
    "\n",
    "    cv_scores = model_selection.cross_validate(\n",
    "        classifier, embeddings, labels, scoring=scoring, cv=shuffle,\n",
    "        return_train_score=True)\n",
    "    \n",
    "    train_acc = cv_scores['train_accuracy'].mean()\n",
    "    train_macro_f1 = cv_scores['train_f1_macro'].mean()\n",
    "    train_micro_f1 = cv_scores['train_f1_micro'].mean()\n",
    "    test_acc = cv_scores['test_accuracy'].mean()\n",
    "    test_macro_f1 = cv_scores['test_f1_macro'].mean()\n",
    "    test_micro_f1 = cv_scores['test_f1_micro'].mean()\n",
    "\n",
    "    print(\"Train acc: {:0.4f}, macro_f1: {:0.4f}, micro_f1: {:0.4f}\".format(train_acc, train_macro_f1,train_micro_f1))\n",
    "    print(\"Test acc: {:0.4f}, macro_f1: {:0.4f}, micro_f1: {:0.4f}\".format(test_acc, test_macro_f1,test_micro_f1))\n",
    "\n",
    "    return {'train_acc': train_acc, 'test_acc': test_acc, 'train_macro_f1': train_macro_f1, 'test_macro_f1': test_macro_f1, \n",
    "            'train_micro_f1': train_micro_f1, 'test_micro_f1': test_micro_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function writes the final f1 scores to output file.\n",
    "def save_scores(scores, prefix):\n",
    "    with open(\"scores_fourone.txt\",\"a\") as f:\n",
    "        f.write(\"{:0.4f}, {:0.4f}, {:0.4f}, {:0.4f}, {:0.4f}, {:0.4f}\\n\"\n",
    "                .format(scores[\"train_acc\"], scores[\"train_macro_f1\"], scores[\"train_micro_f1\"],scores[\"test_acc\"],\n",
    "                        scores[\"test_macro_f1\"],scores[\"test_micro_f1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 7624\n"
     ]
    }
   ],
   "source": [
    "# This code reads the embedding and sorts based on the node id. So the label data can be matched and used for evaluation.!!\n",
    "embeddings = []\n",
    "emb_dict = {}\n",
    "all_embeddings = []\n",
    "with open(\"emb_dgl_fourone.txt\", \"r\") as pfile:\n",
    "    for line in pfile:\n",
    "        line = line[:-1]\n",
    "        line = line.split(' ')\n",
    "        embeddings.append(line)\n",
    "embeddings = embeddings[1:]\n",
    "\n",
    "for row in embeddings:\n",
    "    emb_dict[int(row[0])] = [float(x) for x in row[1:]]\n",
    "del embeddings\n",
    "\n",
    "for i in range(0,len(emb_dict)):\n",
    "    all_embeddings.append(emb_dict[i])\n",
    "del emb_dict\n",
    "v_size = len(all_embeddings)\n",
    "print(\"Vocab size: {}\".format(v_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Labels: 7624\n",
      "{0: 1098, 1: 54, 2: 73, 3: 515, 4: 16, 5: 391, 6: 655, 7: 82, 8: 468, 9: 58, 10: 1303, 11: 138, 12: 57, 13: 63, 14: 570, 15: 257, 16: 254, 17: 1572}\n",
      "Labeled vocab size: 7624\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Loading labels from the labels file.\n",
    "r_labels, all_labels = load_labels('lastfm_asia_target.txt', v_size)\n",
    "un, counts = np.unique(all_labels, return_counts=True)\n",
    "print(dict(zip(un, counts)))\n",
    "print(\"Labeled vocab size: {}\".format(len(all_labels)))\n",
    "print(all_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing evaluation for different set of split ratio and seed values!!\n",
    "seeds = [58125312,58125333,58125111,58125000]\n",
    "train_split = [0.4,0.3,0.2,0.1]\n",
    "for seed,split in zip(seeds,train_split):\n",
    "    evals = eval_classification(all_labels, all_embeddings,seed,split)\n",
    "    save_scores(evals, \"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
